{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### modules pour le chargement des données depuis le XML ######\n",
    "import glob\n",
    "from lxml import etree\n",
    "from preTraitements.xml import get_X_Y_from_root\n",
    "from preTraitements.xml import get_tree_root_from_file\n",
    "\n",
    "###### modules pour la classification ######\n",
    "\n",
    "# modèles\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# création de nos transformers\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer # créer nos propres transformer\n",
    "\n",
    "# recherche des meilleurs hyperparamètres\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# résultats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# sauvegarde des modèles\n",
    "from joblib import dump, load\n",
    "\n",
    "###### modules pour la visualisation ######\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "###### miscellaneous ######\n",
    "from typing import List # typage des fonctions\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "##### multi class #####\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af78629",
   "metadata": {},
   "source": [
    "# Glove vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b24236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "#root folder\n",
    "root_folder='.'\n",
    "glove_filename='vectors.txt'\n",
    "\n",
    "# Variable for data directory\n",
    "glove_path = os.path.abspath(glove_filename)\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "#remplace un CountVectorizer()\n",
    "def GloveVectorizer(X_train):\n",
    "    n=0\n",
    "    m = glove.get_vector('président')\n",
    "    docs = np.zeros((len(X_train),m.shape[0]))\n",
    "    for document in X_train:\n",
    "        word_vectors=[]\n",
    "        for token in document.split():\n",
    "            if token in glove:\n",
    "                word_vectors.append(glove[token])\n",
    "        if len(word_vectors)>0:\n",
    "            word_vectors = np.array(word_vectors)\n",
    "            docs[n]=word_vectors.mean(axis=0)\n",
    "        else:\n",
    "            docs[n]=np.zeros(m.shape[0])              \n",
    "        n=n+1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe8ffa",
   "metadata": {},
   "source": [
    "# w2vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_filename='text8-vector.bin'\n",
    "\n",
    "# Variable for data directory\n",
    "w2v_path = os.path.abspath(w2v_filename)\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "def w2vVectorizer(X_train):\n",
    "    n=0\n",
    "    m = wv_from_bin.get_vector('président')\n",
    "    docs = np.zeros((len(X_train),m.shape[0]))\n",
    "    for document in X_train:\n",
    "        word_vectors=[]\n",
    "        for token in document.split():\n",
    "            if token in wv_from_bin:\n",
    "                word_vectors.append(wv_from_bin[token])\n",
    "        if len(word_vectors)>0:\n",
    "            word_vectors = np.array(word_vectors)\n",
    "            docs[n]=word_vectors.mean(axis=0)\n",
    "        else:\n",
    "            docs[n]=np.zeros(m.shape[0])          \n",
    "        n=n+1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa182a4",
   "metadata": {},
   "source": [
    "# FastText vector\n",
    "\n",
    "Les résultats sont pas meilleurs que pour w2v, je vous mets quand même ce que j'ai fait. J'ai pas pu importer les vecteurs parce que trop lourd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93ae9c",
   "metadata": {},
   "source": [
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "fde = 'cc.fr.300.bin'\n",
    "ft_path = os.path.abspath(fde)\n",
    "print(ft_path)\n",
    "ft = load_facebook_vectors(fde)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c355",
   "metadata": {},
   "source": [
    "```python\n",
    "def FTVectorizer(X_train):\n",
    "    n=0\n",
    "    m = ft.get_vector('président')\n",
    "    docs = np.zeros((len(X_train),m.shape[0]))\n",
    "    for document in X_train:\n",
    "        word_vectors=[]\n",
    "        for token in document.split():\n",
    "            if token in ft:\n",
    "                word_vectors.append(ft[token])\n",
    "        if len(word_vectors)>0:\n",
    "            word_vectors = np.array(word_vectors)\n",
    "            docs[n]=word_vectors.mean(axis=0)\n",
    "        else:\n",
    "            docs[n]=np.zeros(m.shape[0])              \n",
    "        n=n+1\n",
    "    return docs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269cfb3",
   "metadata": {},
   "source": [
    "# Les meilleures pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55152e2",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_r = Pipeline([\n",
    "        ('ngram_tf_idf', Pipeline([\n",
    "          ('counts', FunctionTransformer(w2vVectorizer)),\n",
    "          ('tf_idf', TfidfTransformer(use_idf=False))\n",
    "        ])),\n",
    "  ('standard', StandardScaler(with_mean=False)),\n",
    "  ('r',RandomForestClassifier(criterion='gini',max_features='sqrt'))])\n",
    "\n",
    "\n",
    "liste_rfc.append(pipeline_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa1a40",
   "metadata": {},
   "source": [
    "# SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6006784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(CountVectorizer(),TfidfTransformer(),StandardScaler(with_mean=False),\n",
    "                    OneVsOneClassifier(SGDClassifier(max_iter=1000, tol=1e-3)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
